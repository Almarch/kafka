{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99e6496-2ab0-464c-a1cf-f7de01059b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b091bb-9a71-4ee7-ada8-b874a70f13c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15494\n"
     ]
    }
   ],
   "source": [
    "with open(\"chateau.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# remove page numbers\n",
    "txt = re.sub(r\"–\\s*\\d+\\s*–\\n\", \"\", txt)\n",
    "\n",
    "# fix split words\n",
    "txt = re.sub(r\"-\\n\", \"\", txt)\n",
    "\n",
    "# remove line breaks\n",
    "txt = re.sub(r\"\\n\", \" \", txt)\n",
    "\n",
    "# use a single type of -\n",
    "txt = re.sub(r\"–\", \"-\", txt)\n",
    "\n",
    "with open(\"clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(txt)\n",
    "\n",
    "print(len(set(txt.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56297c4d-eaf0-4693-a9dd-d0e051a0c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tokenizable.txt\n",
      "  input_format: \n",
      "  model_prefix: tok\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 512\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: tokenizable.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 3826 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=689484\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=94\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 3826 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=359044\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 29084 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 3826\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 15494\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 15494 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9442 obj=11.2226 num_tokens=33198 num_tokens/piece=3.51599\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8143 obj=8.96044 num_tokens=33365 num_tokens/piece=4.09738\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6104 obj=9.0441 num_tokens=35689 num_tokens/piece=5.84682\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6102 obj=9.00362 num_tokens=35687 num_tokens/piece=5.84841\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4575 obj=9.30053 num_tokens=39497 num_tokens/piece=8.63322\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4575 obj=9.24501 num_tokens=39506 num_tokens/piece=8.63519\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3431 obj=9.55916 num_tokens=43956 num_tokens/piece=12.8114\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3431 obj=9.49818 num_tokens=43959 num_tokens/piece=12.8123\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2573 obj=9.89622 num_tokens=48869 num_tokens/piece=18.993\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2573 obj=9.83264 num_tokens=48867 num_tokens/piece=18.9922\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1929 obj=10.2819 num_tokens=54005 num_tokens/piece=27.9964\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1929 obj=10.2177 num_tokens=54002 num_tokens/piece=27.9948\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1446 obj=10.814 num_tokens=59483 num_tokens/piece=41.1362\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1446 obj=10.738 num_tokens=59485 num_tokens/piece=41.1376\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1084 obj=11.2944 num_tokens=64811 num_tokens/piece=59.7887\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1084 obj=11.2178 num_tokens=64813 num_tokens/piece=59.7906\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=813 obj=11.961 num_tokens=69461 num_tokens/piece=85.4379\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=813 obj=11.875 num_tokens=69461 num_tokens/piece=85.4379\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=609 obj=12.6458 num_tokens=74149 num_tokens/piece=121.755\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=609 obj=12.5362 num_tokens=74174 num_tokens/piece=121.796\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=563 obj=12.8243 num_tokens=75696 num_tokens/piece=134.451\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=563 obj=12.7953 num_tokens=75695 num_tokens/piece=134.449\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tok.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tok.vocab\n"
     ]
    }
   ],
   "source": [
    "with open(\"clean.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tmp = f.read()\n",
    "\n",
    "tmp = re.sub(r\"\\s-\\s\", \"\\n- \", txt)\n",
    "tmp = re.sub(r\"(?<!K)\\.\\s\", \".\\n\", txt)\n",
    "\n",
    "with open(\"tokenizable.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(tmp)\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input = \"tokenizable.txt\",\n",
    "    model_prefix = \"tok\",\n",
    "    vocab_size = 512,\n",
    "    model_type = \"unigram\",\n",
    "    character_coverage = 1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d72a6e9-507e-4731-b512-8be3f0fa2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=\"tok.model\")\n",
    "tokens = sp.encode(txt, out_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "030fdf04-9c33-4f4e-b1ea-b1cc37157e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = len(sp)\n",
    "        self.attention_window = 64\n",
    "        self.nheads = 6\n",
    "        self.d_model = self.nheads * 8\n",
    "        self.mlp_size = 4 * self.d_model\n",
    "        self.n_attention_layers = 3\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, self.attention_window, self.d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.nheads,\n",
    "            dim_feedforward=self.mlp_size,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=self.n_attention_layers,\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(self.d_model, self.vocab_size, bias=False)\n",
    "        self.out.weight = self.embed.weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) + self.pos[:, :x.size(1)]\n",
    "        mask = self._causal_mask(x.size(1), x.device)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        return self.out(x)\n",
    "\n",
    "    def _causal_mask(self, size, device):\n",
    "        mask = torch.triu(torch.ones(size, size, device=device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "model = GPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c406ce6-f7b9-4187-bfc3-49ff249eb273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- window 1 ---\n",
      "Il était tard lorsque K. arriva. Une neige épaisse couvrait le village. La colline était cachée par la brume et par la nuit, nul rayon de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "--- window 2 ---\n",
      "lumière n’indiquait le grand Château. K. resta longtemps sur le pont de bois qui menait de la grand-route au village, les yeux levés vers ces hauteurs qui sembbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "\n",
      "--- window 3 ---\n",
      "laient vides. Puis il alla chercher un gîte ; les gens de l’auberge n’étaient pas encore au lit ; on n’avait pas de chambre à louer, mais, surpris et déconcerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(3):\n",
    "    start = i * 64\n",
    "    end = start + 64\n",
    "    subset = tokens[start:end]\n",
    "    x = torch.tensor([subset], dtype=torch.long)\n",
    "    for _ in range(32):\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "        next_id = torch.argmax(logits[0, -1]).item()\n",
    "        subset.append(next_id)\n",
    "        x = torch.tensor([subset[-64:]], dtype=torch.long)\n",
    "\n",
    "    print(f\"\\n--- window {i+1} ---\")\n",
    "    print(sp.decode(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ad7102-cc91-43c5-9bf9-85aac92ba3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch 1 | loss=7.0911\n",
      "Epoch 2 | loss=5.3308\n",
      "Epoch 3 | loss=5.2791\n",
      "Epoch 4 | loss=5.2607\n",
      "Epoch 5 | loss=5.2532\n",
      "Epoch 6 | loss=5.2480\n",
      "Epoch 7 | loss=5.2453\n",
      "Epoch 8 | loss=5.2420\n",
      "Epoch 9 | loss=5.2392\n",
      "Epoch 10 | loss=5.2380\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "seq_in, seq_out = 64, 32\n",
    "stride = 32\n",
    "\n",
    "inputs, targets = [], []\n",
    "for i in range(0, len(tokens) - seq_in - seq_out, stride):\n",
    "    chunk = tokens[i : i + seq_in + seq_out]\n",
    "    inputs.append(chunk[:seq_in])\n",
    "    targets.append(chunk[seq_in:])\n",
    "\n",
    "X = torch.tensor(inputs, dtype=torch.long)\n",
    "Y = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for Xb, Yb in loader:\n",
    "        Xb, Yb = Xb.to(device), Yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xb)\n",
    "        loss = criterion(\n",
    "            logits[:, -seq_out:, :].reshape(-1, model.vocab_size),\n",
    "            Yb.reshape(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | loss={total_loss/len(loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"gpt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c481397-b441-46b0-a7ee-afa032de66e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- window 1 ---\n",
      "Il était tard lorsque K. arriva. Une neige épaisse couvrait le village. La colline était cachée par la brume et par la nuit, nul rayon de                                \n",
      "\n",
      "--- window 2 ---\n",
      "lumière n’indiquait le grand Château. K. resta longtemps sur le pont de bois qui menait de la grand-route au village, les yeux levés vers ces hauteurs qui semb                       sssssssss\n",
      "\n",
      "--- window 3 ---\n",
      "laient vides. Puis il alla chercher un gîte ; les gens de l’auberge n’étaient pas encore au lit ; on n’avait pas de chambre à louer, mais, surpris et déconcerssssssssssssssssssssssssssssssss\n"
     ]
    }
   ],
   "source": [
    "model = GPT()\n",
    "model.load_state_dict(torch.load(\"gpt.pt\"))\n",
    "model.eval()\n",
    "\n",
    "for i in range(3):\n",
    "    start = i * 64\n",
    "    end = start + 64\n",
    "    subset = tokens[start:end]\n",
    "    x = torch.tensor([subset], dtype=torch.long)\n",
    "    for _ in range(32):\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "        next_id = torch.argmax(logits[0, -1]).item()\n",
    "        subset.append(next_id)\n",
    "        x = torch.tensor([subset[-64:]], dtype=torch.long)\n",
    "\n",
    "    print(f\"\\n--- window {i+1} ---\")\n",
    "    print(sp.decode(subset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99e6496-2ab0-464c-a1cf-f7de01059b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df714a8d-cae8-42b3-8e8b-ccbccd1a566e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d790af4-60cb-4852-9590-61e1c131790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Total number of parameters : 1286528\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = 2048\n",
    "        self.attention_window = 256\n",
    "        self.nheads = 8\n",
    "        self.d_model = self.nheads * 16\n",
    "        self.mlp_size = 4 * self.d_model\n",
    "        self.n_attention_layers = 5\n",
    "        self.dropout = 0.1\n",
    "        self.activation = \"gelu\"\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, self.attention_window, self.d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.nheads,\n",
    "            dim_feedforward=self.mlp_size,\n",
    "            activation= self.activation,\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=self.n_attention_layers,\n",
    "            norm=nn.LayerNorm(self.d_model),\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(self.d_model, self.vocab_size, bias=False)\n",
    "        self.out.weight = self.embed.weight\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) + self.pos[:, :x.size(1)]\n",
    "        mask = self._causal_mask(x.size(1), x.device)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        return self.out(x)\n",
    "\n",
    "    def _causal_mask(self, size, device):\n",
    "        mask = torch.triu(torch.ones(size, size, device=device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "model = GPT()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters : {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad515b86-5c2e-41a6-ad06-d70a34582bce",
   "metadata": {},
   "source": [
    "# Heavy tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680d7b8-7547-4e83-ac1e-143e310c4f4f",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47aaa5d6-1af9-462f-81ab-91d455cd7b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 52/52 [28:41<00:00, 33.11s/files]\n",
      "Generating de split: 100%|██████████| 3131/3131 [00:08<00:00, 380.52 examples/s]\n",
      "Generating en split: 100%|██████████| 61340/61340 [01:57<00:00, 523.31 examples/s]\n",
      "Generating es split: 100%|██████████| 1202/1202 [00:02<00:00, 451.13 examples/s]\n",
      "Generating fr split: 100%|██████████| 5493/5493 [00:10<00:00, 526.31 examples/s]\n",
      "Generating it split: 100%|██████████| 1008/1008 [00:01<00:00, 640.86 examples/s]\n",
      "Generating nl split: 100%|██████████| 1420/1420 [00:15<00:00, 93.87 examples/s] \n",
      "Generating pl split: 100%|██████████| 34/34 [00:00<00:00, 2181.33 examples/s]\n",
      "Generating pt split: 100%|██████████| 1111/1111 [00:00<00:00, 1521.69 examples/s]\n",
      "Generating ru split: 100%|██████████| 6/6 [00:00<00:00, 1316.62 examples/s]\n",
      "Generating sv split: 100%|██████████| 388/388 [00:00<00:00, 921.52 examples/s]\n",
      "Generating zh split: 100%|██████████| 437/437 [00:00<00:00, 470.27 examples/s]\n",
      "Saving the dataset (5/5 shards): 100%|██████████| 5493/5493 [00:30<00:00, 179.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "gutenberg = load_dataset(\"manu/project_gutenberg\", split=\"fr\", streaming=False)\n",
    "gutenberg.save_to_disk(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a493fe-5cf2-4129-820e-a01b8549beb3",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034eab04-86df-46c8-a009-747e34f049ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"gutenberg\")\n",
    "with open(\"gutenberg.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in ds:\n",
    "        txt = ex.get(\"text\") or ex.get(\"content\") or \"\"\n",
    "        f.write(txt + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "432230cb-4303-40e1-ae41-720520feb243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: gutenberg.txt\n",
      "  input_format: \n",
      "  model_prefix: tok\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2048\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.999\n",
      "  input_sentence_size: 2000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: gutenberg.txt\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 3000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 4000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 5000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 6000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 7000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 8000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 9000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 10000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 11000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 12000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 13000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 14000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 15000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 16000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 17000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 18000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 19000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 20000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 21000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 22000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 23000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 24000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 25000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 26000000 lines\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (4307 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 27000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 28000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 29000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 30000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 31000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 32000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 33000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 34000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 35000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 36000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 37000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (2000000), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(411) LOG(INFO) Sampled 2000000 sentences from 37325611 sentences.\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 4 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=120170049\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9059% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=95\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999059\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1999997 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=62444043\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 809266 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1999997\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 884699\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 884699 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=306246 obj=12.6859 num_tokens=2231538 num_tokens/piece=7.28675\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=271054 obj=10.0441 num_tokens=2237178 num_tokens/piece=8.25362\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=203261 obj=10.0182 num_tokens=2307790 num_tokens/piece=11.3538\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=203186 obj=10.0091 num_tokens=2308036 num_tokens/piece=11.3592\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=152389 obj=10.0544 num_tokens=2412221 num_tokens/piece=15.8294\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=152389 obj=10.0423 num_tokens=2412031 num_tokens/piece=15.8281\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=114291 obj=10.0993 num_tokens=2526562 num_tokens/piece=22.1064\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=114291 obj=10.0881 num_tokens=2526313 num_tokens/piece=22.1042\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=85718 obj=10.1662 num_tokens=2645820 num_tokens/piece=30.8666\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=85718 obj=10.1526 num_tokens=2645619 num_tokens/piece=30.8642\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=64288 obj=10.2459 num_tokens=2768464 num_tokens/piece=43.0635\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=64288 obj=10.2283 num_tokens=2768453 num_tokens/piece=43.0633\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=48216 obj=10.3466 num_tokens=2894888 num_tokens/piece=60.04\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=48216 obj=10.3245 num_tokens=2894922 num_tokens/piece=60.0407\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=36162 obj=10.4683 num_tokens=3025354 num_tokens/piece=83.6611\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=36162 obj=10.4407 num_tokens=3025468 num_tokens/piece=83.6643\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=27121 obj=10.6148 num_tokens=3158658 num_tokens/piece=116.465\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=27121 obj=10.5814 num_tokens=3158970 num_tokens/piece=116.477\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=20340 obj=10.7794 num_tokens=3303051 num_tokens/piece=162.392\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20340 obj=10.7392 num_tokens=3304044 num_tokens/piece=162.441\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=15255 obj=10.9729 num_tokens=3450565 num_tokens/piece=226.192\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=15255 obj=10.9246 num_tokens=3452765 num_tokens/piece=226.337\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11441 obj=11.1951 num_tokens=3605154 num_tokens/piece=315.108\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11441 obj=11.1378 num_tokens=3606360 num_tokens/piece=315.214\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8580 obj=11.4434 num_tokens=3763859 num_tokens/piece=438.678\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8580 obj=11.3774 num_tokens=3764035 num_tokens/piece=438.699\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6435 obj=11.7289 num_tokens=3930721 num_tokens/piece=610.835\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6435 obj=11.6552 num_tokens=3930869 num_tokens/piece=610.858\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4826 obj=12.042 num_tokens=4095239 num_tokens/piece=848.578\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4826 obj=11.9612 num_tokens=4094927 num_tokens/piece=848.514\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3619 obj=12.3792 num_tokens=4269348 num_tokens/piece=1179.7\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3619 obj=12.2913 num_tokens=4268732 num_tokens/piece=1179.53\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2714 obj=12.7371 num_tokens=4451113 num_tokens/piece=1640.06\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2714 obj=12.6426 num_tokens=4451061 num_tokens/piece=1640.04\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2252 obj=12.933 num_tokens=4574366 num_tokens/piece=2031.25\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2252 obj=12.8736 num_tokens=4574581 num_tokens/piece=2031.34\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tok.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tok.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"gutenberg.txt\",\n",
    "    model_prefix=\"tok\",\n",
    "    vocab_size=model.vocab_size,\n",
    "    model_type=\"unigram\",\n",
    "    character_coverage=0.999,\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    input_sentence_size=2_000_000,\n",
    "    train_extremely_large_corpus=True,\n",
    "    shuffle_input_sentence = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2fe6b9-cb84-4b69-a095-fdca9a75e02b",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf4c044e-daf4-478f-9592-a7319ba9e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=\"tok.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d3dea-4f9d-46f5-af16-66f07abfc3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "seq_in, seq_out = model.attention_window, model.attention_window // 2\n",
    "stride = seq_out\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0,\n",
    ")\n",
    "\n",
    "log_path = \"gutenberg.log\"\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as log:\n",
    "    log.write(\"epoch,step,loss\\n\")\n",
    "\n",
    "def stream_batches(path, sp, seq_in, seq_out, batch_size=16):\n",
    "    buf = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ids = sp.encode(line.strip(), out_type=int)\n",
    "            buf.extend(ids + [sp.eos_id()])\n",
    "            while len(buf) >= seq_in + seq_out:\n",
    "                chunk = buf[: seq_in + seq_out]\n",
    "                buf = buf[seq_out:]\n",
    "                X = torch.tensor(chunk[:seq_in], dtype=torch.long)\n",
    "                Y = torch.tensor(chunk[seq_in:], dtype=torch.long)\n",
    "                yield X, Y\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    step = 0\n",
    "    batch_X, batch_Y = [], []\n",
    "\n",
    "    for X, Y in stream_batches(\"gutenberg.txt\", sp, seq_in, seq_out):\n",
    "        batch_X.append(X)\n",
    "        batch_Y.append(Y)\n",
    "        if len(batch_X) == 16:\n",
    "            Xb = torch.stack(batch_X).to(device)\n",
    "            Yb = torch.stack(batch_Y).to(device)\n",
    "            batch_X.clear()\n",
    "            batch_Y.clear()\n",
    "            step += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(Xb)\n",
    "            loss = criterion(\n",
    "                logits[:, -seq_out:, :].reshape(-1, model.vocab_size),\n",
    "                Yb.reshape(-1)\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                avg_loss = total_loss / step\n",
    "                with open(log_path, \"a\", encoding=\"utf-8\") as log:\n",
    "                    log.write(f\"{epoch+1},{step},{avg_loss:.10f}\\n\")\n",
    "                    \n",
    "            if step >= 10_000 and optimizer.param_groups[0][\"lr\"] > 1e-4:\n",
    "                for g in optimizer.param_groups:\n",
    "                    g[\"lr\"] = 1e-4\n",
    "                \n",
    "torch.save(model.state_dict(), f\"gpt_gutenberg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04263aad-e290-4372-be6e-dc6495d02e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "temperature = 0.8\n",
    "\n",
    "starters = []\n",
    "for X, _ in stream_batches(\"gutenberg.txt\", sp, seq_in, seq_out):\n",
    "    starters.append(X.tolist())\n",
    "    if len(starters) >= 3:\n",
    "        break\n",
    "\n",
    "for i, subset in enumerate(starters):\n",
    "    subset = subset.copy()\n",
    "    x = torch.tensor([subset], dtype=torch.long, device=device)\n",
    "    for _ in range(200):\n",
    "        x = torch.tensor([subset[-model.attention_window:]], dtype=torch.long, device=device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "        probs = torch.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "        subset.append(next_id)\n",
    "    print(f\"\\n[SAMPLE {i+1}]\\n\", sp.decode(subset), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2282ec3-ddca-4585-a40e-d4798c0771da",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d9a78-141c-47ca-abb7-e8435b7bcc7a",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b091bb-9a71-4ee7-ada8-b874a70f13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chateau.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# remove page numbers\n",
    "txt = re.sub(r\"–\\s*\\d+\\s*–\\n\", \"\", txt)\n",
    "\n",
    "# fix split words\n",
    "txt = re.sub(r\"-\\n\", \"\", txt)\n",
    "\n",
    "# remove line breaks\n",
    "txt = re.sub(r\"\\n\", \" \", txt)\n",
    "\n",
    "# use a single type of -\n",
    "txt = re.sub(r\"–\", \"-\", txt)\n",
    "\n",
    "with open(\"clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(txt)\n",
    "\n",
    "print(len(set(txt.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72a6e9-507e-4731-b512-8be3f0fa2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=\"tok.model\")\n",
    "tokens = sp.encode(txt, out_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad7102-cc91-43c5-9bf9-85aac92ba3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_in, seq_out = model.attention_window, model.attention_window // 2\n",
    "stride = seq_out\n",
    "\n",
    "inputs, targets = [], []\n",
    "for i in range(0, len(tokens) - seq_in - seq_out, stride):\n",
    "    chunk = tokens[i : i + seq_in + seq_out]\n",
    "    inputs.append(chunk[:seq_in])\n",
    "    targets.append(chunk[seq_in:])\n",
    "\n",
    "X = torch.tensor(inputs, dtype=torch.long)\n",
    "Y = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-5,\n",
    "    weight_decay=0,\n",
    ")\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for Xb, Yb in loader:\n",
    "        Xb, Yb = Xb.to(device), Yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xb)\n",
    "        loss = criterion(\n",
    "            logits[:, -seq_out:, :].reshape(-1, model.vocab_size),\n",
    "            Yb.reshape(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if(epoch + 1 < 10):\n",
    "        print(f\"Epoch {epoch+1} | loss={total_loss/len(loader):.5f}\")\n",
    "        \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1} | loss={total_loss/len(loader):.5f}\")\n",
    "        model.eval()\n",
    "        subset = tokens[:model.attention_window]\n",
    "        x = torch.tensor([subset], dtype=torch.long)\n",
    "        for _ in range(200):\n",
    "            x = torch.tensor([subset[-model.attention_window:]], dtype=torch.long, device=device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(x)\n",
    "            probs = torch.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "            next_id = torch.multinomial(probs, 1).item()\n",
    "            subset.append(next_id)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(\"\\n[SAMPLE]\", sp.decode(subset), \"\\n\")\n",
    "        torch.save(model.state_dict(), f\"gpt_epoch_{epoch+1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226106b8-a258-48e3-aa0e-d7b457cd5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "temperature = 0.8\n",
    "\n",
    "for i in range(3):\n",
    "    start = i * model.attention_window\n",
    "    end = start + model.attention_window\n",
    "    subset = tokens[start:end]\n",
    "    x = torch.tensor([subset], dtype=torch.long)\n",
    "    for _ in range(200):\n",
    "        x = torch.tensor([subset[-model.attention_window:]], dtype=torch.long, device=device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "        probs = torch.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "        subset.append(next_id)\n",
    "    print(\"\\n[SAMPLE]\", sp.decode(subset), \"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

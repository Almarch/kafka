{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2558a19-d5ec-4734-9633-c5cbc297faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffbdf3-b611-4381-a08c-96dde9dfec6b",
   "metadata": {},
   "source": [
    "## Load the model as 4-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d17ed25-b1e6-4dc7-bf73-39247fda7012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d79ad01e2247369c275ec06e21fd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"openllama_f16\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa58fbd-edab-4059-af0f-1e8c80c3d986",
   "metadata": {},
   "source": [
    "## Preparation for QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e6038c-0f91-4898-b814-a4b3234f00eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 101,703,680 || all params: 3,528,177,280 || trainable%: 2.8826\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64, # rank\n",
    "    lora_alpha=16, # scaling factor\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05, # 5% activation randomly set to 0 during training\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # next token\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f6efc67-3423-4247-a74d-7ee4ac441dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_dataset(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = [json.loads(line)[\"text\"] for line in f]\n",
    "    \n",
    "    tokens = tokenizer(texts)\n",
    "    tokens[\"labels\"] = [ids.copy() for ids in tokens[\"input_ids\"]]\n",
    "    \n",
    "    return Dataset.from_dict(tokens)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"kafka_openllama\",\n",
    "    \n",
    "    # Batch & Accumulation\n",
    "    per_device_train_batch_size=1,        # Real batch size per GPU\n",
    "    gradient_accumulation_steps=32,       # Accumulate 32 steps → effective batch = 32\n",
    "    \n",
    "    # Epochs control\n",
    "    num_train_epochs=1,                   # 1 epoch\n",
    "    max_steps=-1,                         # -1 = use num_train_epochs instead of fixed steps\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=1e-5,                   # Max LR (will be modulated by scheduler)\n",
    "    lr_scheduler_type=\"cosine\",           # Cosine annealing: smooth decay from max to 0\n",
    "    warmup_ratio=0.03,                    # 3% of steps for warmup (prevents initial shock)\n",
    "    \n",
    "    # Optimizer\n",
    "    optim=\"paged_adamw_8bit\",             # 8-bit Adam: saves ~20% VRAM vs standard Adam\n",
    "    \n",
    "    # Gradient clipping\n",
    "    max_grad_norm=0.3,                    # Clip gradients to prevent exploding gradients\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,                            # Use bfloat16 (Ampere+ GPUs: A100, RTX 30xx+)\n",
    "    fp16=False,                           # Don't use float16 (bf16 is better)\n",
    "    \n",
    "    # Logging & Checkpoints\n",
    "    logging_steps=50,                     # Print logs every 50 steps\n",
    "    save_steps=500,                       # Save checkpoint every 500 steps\n",
    "    save_total_limit=3,                   # Keep only 3 latest checkpoints (saves disk space)\n",
    "    \n",
    "    # DataLoader optimizations\n",
    "    dataloader_pin_memory=True,           # Faster GPU transfers (if enough RAM)\n",
    "    dataloader_num_workers=2,             # Parallel data loading (2 CPU threads)\n",
    "    remove_unused_columns=False,          # Don't auto-remove columns (we handle it manually)\n",
    "    \n",
    "    # Monitoring\n",
    "    report_to=\"none\"                      # No WandB/TensorBoard (set \"tensorboard\" if needed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3f37fb-8731-4641-8e9c-044b41b014c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2078/3297851702.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='184' max='184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [184/184 2:13:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.080100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('kafkallama/tokenizer_config.json',\n",
       " 'kafkallama/special_tokens_map.json',\n",
       " 'kafkallama/tokenizer.model',\n",
       " 'kafkallama/added_tokens.json',\n",
       " 'kafkallama/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_jsonl_dataset(\"train.jsonl\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "final_path = \"kafkallama\"\n",
    "model.save_pretrained(final_path)\n",
    "tokenizer.save_pretrained(final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6a25d75-814d-4a01-b4d7-53040e674100",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b7e79-d612-4c63-aa86-3f3ae5e5162f",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81cb5d45-23d3-4b40-aba6-c07db6f0303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== generation 0 ==\n",
      "K. ouvrit la porte. 12:45, le dimanche\n",
      "Je vais au cinéma avec les jeunes. Il y aura des snacks! (Pour ma famille)\n",
      "\n",
      "\n",
      "== generation 1 ==\n",
      "K. ouvrit la porte. 16503, The Gnostic Bible (2 vols), London : SPCK\n",
      "SPECIAL OFFER: Paperback £9.47 + FREE P&P worldwide on our webstore!\n",
      "\n",
      "\n",
      "== generation 2 ==\n",
      "K. ouvrit la porte. 01:52 ------------------------------- #PokemonSwordAndShield#MewtwoIsMyFavoriteHero Pikachu and Squirtle are now on YouTube! Don't forget to SUBSCRIBE, like this video, share it with your friends or leave a comment below :D\n",
      "\n",
      "\n",
      "== generation 3 ==\n",
      "K. ouvrit la porte. À quelques pas, en face du foyer d’Hervé et de son épouse, il se dirigea vers le sien.\n",
      "“Il y a des choses qui nous changent, poursuivit-il avec un air de dépit. “Tout à coup on ne peut plus rien faire pour les autres.” Et puis: “Il faut leur parler bien clairement de toutes les choses que tu es prêt à faire pour eux”… Il se leva de nouveau et alla s’asseoir dans une armoire dont l’extrémité se trouvait à peine plus loin que le bord de la cheminée. Mais il n’y enfonça pas un bras ; sans attendre, il se mit à lire.\n",
      "Kaiya had come into the house quietly and without disturbing Hervé or his wife. He took her by surprise as he was reading, but quickly recovered himself to greet them with a smile of greeting upon seeing their faces; it did not seem to occur to him that they were angry at having been woken from sleep so early in the morning. She had only just begun to speak when she heard someone call out for him – who knew how far away that voice must have been? It didn’t matter, really, because this time around there was no need to rush through anything. No need to hurry past other people before you reached your own door. And still more pleasantly, since none of these distractions would be present if she went inside, everything felt simpler. This way too, each step along the path felt like one less obstacle between herself and where she wanted to go. Everything here seemed smaller than it should have been; yet all at once it became easier. The world was no longer filled with obstacles; instead its boundaries were merely defined by walls – solid, invisible ones that made up every room in the house. She could feel things becoming larger, yet again, somehow. They were able to be seen clearly now, even though some of them were still obscured by shadows. She could see the entirety of what stood in front of her. What lay beyond the edge of view was still out of reach. But perhaps it was possible to reach that place after all, which is why today wasn’t a day like any other. Her eyes were fixed on something else, somewhere in the distance. There was an endless space behind those barriers, a space where nothing existed. Yet this time around, there was a limitless freedom, as well as something approaching happiness..\n",
      "\n",
      "\n",
      "== generation 4 ==\n",
      "K. ouvrit la porte. 64,50 € Livraison : Délai de livraison : .\n",
      "Temps d'attente : en cours...\n",
      "\n",
      "\n",
      "== generation 5 ==\n",
      "K. ouvrit la porte. 294035_1876\n",
      "- In reply to: Re: Genealogy of the Sourdeau FamilyKathleen Gauthier 2/17/02\n",
      "\n",
      "\n",
      "== generation 6 ==\n",
      "K. ouvrit la porte. \n",
      "Ils s'en allaient, et à peine le premier aperçut sa silhouette dans la lumière, il voulut se retirer en s'échappant de son groupe pour se cacher du regard d'Elise ; mais elle, n'ayant pas reconnu ses amis, ne s'aperceva que de lui quand une odeur de goutte de vin s'empara de son cœur. Elle s'arrêta un instant, jusqu'à ce qu'elle fut assaillie par les pensées qui lui appelaient : « Pardon, ma chère, t'as été si fidèle à nos années de jeunesse. » Il se retourna pour aller les voir partir, mais elle le faisait peu à peu dévier, car il la regardait comme une vierge qui n'est pas encore commune avec des hommes. Sa voix seule lui répandait un peu d'amertume, bien que, plus avant que rien ne le cachât, il fût en état de s'assurer que cette découverte ne lui avait rien donné mal. Mais qu'est-ce qui la rendait si jalouse ?\n",
      "\n",
      "Cependant, il s'était dit qu'il devait l'entraîner, car elle avait tellement l'air effrayée de le rencontrer. Le plus qu'il y eût de difficultés, c'était pour lui. Qu'importait quoi que ce soit pour quelqu'un qui avait tant de choses à découvrir sur sa propre vie, en dépit de sa grande prudence ? Car elle était une personne qui aussi se sentait obligée d'ouvrir les yeux à la nouveauté, car, au lieu de voir quelques-uns de ses amis à la place de lui, on aurait dû les avoir à leur place ! Même à ce qu'elle y aurait pu croire à la place de son maman. Et elle n'avait pas à se soumettre à leurs injustices. Elle aurait pu les combattre, ces injustices, pour leurs amis.\n",
      "\n",
      "Lorsque la première poussée lui écarta ses bras, lui perdant la main qui venait de lier son étole au cordon, elle hocha la tête et se tourna. Elle regarda son pied droit qui avançait vers lui ; mais, lorsqu'il releva la pointe de sa chaussure en direction de la cheminée, elle se tut. C'était à cause du sentiment que sentait son corps à la suite de la respiration difficile, qui, la veille, lui avait permis de remarquer que sa main n'avait rien vu de particulier. Mais, quoique la bouche ouverte lui paraissait si blanche, qu'on la reconnaissait comme un œuf dans une soupe, il trouva qu'elle avait quelque chose de bleu dans le regard.\n",
      "\n",
      "Il la regarda tout à coup, puis la supplia d'aller se cacher à son tour.\n",
      "\n",
      "« C'est toi, c'est toi, toi, ma petite, toi, toi, toi, toi... »\n",
      "\n",
      "Elle n'y consentit pas. Elle s'essaya, et s'envola au loin, sans lui crier aucun mot.\n",
      "\n",
      "Au bout d'un moment, il la vit revenir.\n",
      "\n",
      "Elle se rassit à nouveau, dans un coin de la pièce, mais son visage n'allait pas au plafond. Mais le spectacle de ses larmes, qui lui traversaient leurs rives, lui fit rire.\n",
      "\n",
      "Elle refusa de pleurer, c'était l'impression la plus ridicule de la vie. Puis, elle lança un sorte de crissement et se redressa.\n",
      "\n",
      "« C'est pas toi, toi, toi, toi... »\n",
      "\n",
      "Sans doute\n",
      "\n",
      "\n",
      "== generation 7 ==\n",
      "K. ouvrit la porte. 7952310648 - The Best Naked Women And Girls Pics, Amateur Porn Pictures\n",
      "Watch Real Teens in the most popular Young Teen Sex and Nude Teen Girl Pics at teenfuns.com Hot young naked girls posing nude for you on camera! Watch Live Cams Now! No Registration Required - % Free Uncensored Adult Chat. Start chatting with amateurs, pornstars, webcam girls and girls online now!.\n",
      "\n",
      "\n",
      "== generation 8 ==\n",
      "K. ouvrit la porte. À peine aperçu, il s'avisa qu'il n'y avait pas encore de moutons, et que l'on ne pouvait pas les entrer sans crier « Bienvenue » au début des moutons.\n",
      "Les rumeurs sont vraies... Et le chemin à suivre pour échapper aux Moulins du Nil se ressemble à une descente dans un abîme en pierre grise. Pour trouver ceux qui ont été fusillés par les autres membres d'une famille, je dois descendre jusqu'au fond de mon puits, où tout est noir, sinon on va rester plusieurs heures sur place avec une tête d'épingle en forme de gouge. C'est prétentieux, mais bien plutôt agréable! Quelque chose comme un drame en cinq actes (quatre réels) : voici deux minutes ; puis un moment de calme, puis trois minutes; toujours deux minutes, et donc soixante minutes de silence. Dans sa maison de campagne, personne n'a jamais rien dit depuis que le monde a commencé à se réveiller, et beaucoup d'hommes ont fini morts. Puis des années ont passé, et nos enfants ont décidé de l'éradiquer. Elle est maintenant une créature très intelligent, capable de faire des choses qu'il n'y a pas d'autre pour le faire. Vous pouvez l'arracher si vous voulez. En tout cas, elle va être extrêmement difficile de la part des autres Moulins du Nil. On est sortis dehors et entraînés à la manière d'un cheval. On y est arrivés. L'attitude de certains êtres humains est inhérente à une situation dangereuse. Ils ne se prennent pas pour des mères. J'ai eu un petit souffle de faim. Après moi, il sera encore beaucoup plus dur. J'y reviens. Je vais à la source et je l'embrasse. Il faut avoir un peu de cœur. Le puits est fermé. La source est le fond d'un mur noir. Sa surface est un verre d'eau. Il n'y a rien qui puisse se perdre dans son couloir. Un gourmand qui vient d'apprendre qu'il n'y a pas de mets dans la nourriture est presque capable de bouger ses jambes. Cela a découvert un peu de bonté. Les gourmands sont extrêmement friands. Elles sont une sorte de moulins. Le poids de la terre dans lesquelles elles sont tombées se rapporte à une quantité de plastique qui ne nécessite pas le moindre souci. Pendant quelques heures, nous avons cherché. Nous étions en train de fumer. Nous avons mis la main dans la glace. Une fois qu'une main a fondu, la seconde ne saurait la retrouver. Il y a des fois, c'est un peu de pire. Nos membres ont commencé à se fracasser. Des fois, le poids de la terre est aussi lourd qu'un corps. Ce qui n'est pas tout à fait le même, mais quand la plupart des gens sont devenus des gens, ça c'est tout le même. Comme la peinture sur toile, c'est la peinture sur toile. Il y a des fois, le peuple n'est pas la peinture. C'est quand la peinture n'est plus que la peinture. C'est quand la peinture n'est plus que la peinture. C'est quand la peinture n'est plus que la peinture. Il y a des fois, le peuple n'est pas la\n",
      "\n",
      "\n",
      "== generation 9 ==\n",
      "K. ouvrit la porte. À une fois, les deux bras l'enfoncèrent dans le couloir. Le frisson lui parvenait de la main gauche, un peu plus frappant que celui qu'il avait déjà ressenti quand il avait ouvert la porte. Elle poussa son chien au bord de la petite piscine, le poignet en l'air, et leva la main pour y chercher l'outil de même.\n",
      "\n",
      "Elle se retourna.\n",
      "\n",
      "Un homme était debout près de la fenêtre et il s'y levait un regard inquiet.\n",
      "\n",
      "Une voix d'adulte roulait dans sa tête.\n",
      "\n",
      "Elle fut vraiment surprise.\n",
      "\n",
      "— Tu vas pas... ? demanda-t-elle.\n",
      "\n",
      "— Oui, mais je...\n",
      "\n",
      "Il se redressa et marcha vers elle.\n",
      "\n",
      "— S'il te plaît.\n",
      "\n",
      "Ella se leva.\n",
      "\n",
      "Il ne semblait pas y avoir de problème.\n",
      "\n",
      "Il fit deux pas et elle eut l'impression que la fenêtre était sur une petite plage de mer. Elle sentit la lumière sur sa peau. Un léger souffle frappa à ses oreilles.\n",
      "\n",
      "Avec un petit cri, elle s'exaspéra.\n",
      "\n",
      "Elle n'osait plus regarder.\n",
      "\n",
      "Elle ne voyait rien.\n",
      "\n",
      "Le couloir était obscur, la baie étroite, tout le monde en dessous, un ventre nu qui lui donnait envie de tomber aux pieds du ciel.\n",
      "\n",
      "C'était si difficile de voir.\n",
      "\n",
      "Trop de lumière.\n",
      "\n",
      "Elle craignait d'avoir malgré tout un objet qu'elle ne reconnaîtrait pas.\n",
      "\n",
      "Cette créature qui lui semblait sortie de ses poches.\n",
      "\n",
      "Elle n'arrivait pas à voir où cela allait.\n",
      "\n",
      "Ses yeux étaient encore dans les flots.\n",
      "\n",
      "Elle se sentit le creux du corps, se sentit sa peau se crocher dans son épaisse tenue de cuir, l'armure de bœufs dont elle était entourée.\n",
      "\n",
      "C'était la force de l'instinct qui lui faisait soudain lever les bras au-dessus du couloir, se mettre dans un grand mouvement pour arriver sur la terrasse et s'immobiliser.\n",
      "\n",
      "Elle resta là, immobile, la tête tournée vers la terre, avec une armure qui s'accrochait à ses épaules.\n",
      "\n",
      "L'horizon devint plus clair.\n",
      "\n",
      "La femme s'arrondissait, ses jambes tendues vers le bas, ses bras se tassaient, les doigts tendus en arrière pour accélérer leur retour vers la terre.\n",
      "\n",
      "Mais quelque chose bloquait sa route.\n",
      "\n",
      "Elle releva les bras, les garda près de ses épaules, des mains appuyées au sol.\n",
      "\n",
      "Elle tremblait.\n",
      "\n",
      "Des mains tremblantes.\n",
      "\n",
      "Quelque chose l'empuja à la renverse.\n",
      "\n",
      "Elle se retourna en arrière, comme si elle eût réussi à rejoindre la terre.\n",
      "\n",
      "Sa peau glissa sur le côté, ses bras s'abaissèrent dans la pénombre et elle se sentit tout à coup assise sur le seuil du couloir.\n",
      "\n",
      "Elle ne savait plus quoi faire.\n",
      "\n",
      "Elle eut une sensation de vertige, de mort.\n",
      "\n",
      "Elle s'accroupit en tâchant de comprendre ce qu'elle n'avait pas vu.\n",
      "\n",
      "Elle se posa sur la droite, ses deux mains qui tremblaient, elle chercha l'objet, et trouva une manche noire, un bout de fil, qui lui remontait au bout de l'index\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_path = \"kafkallama\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"kafkallama\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_path)\n",
    "\n",
    "prompt = \"K. ouvrit la porte. \"\n",
    "\n",
    "for i in range(10):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1000,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"== generation {i} ==\")\n",
    "    print(generated)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

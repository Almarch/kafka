{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99e6496-2ab0-464c-a1cf-f7de01059b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b091bb-9a71-4ee7-ada8-b874a70f13c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15494\n"
     ]
    }
   ],
   "source": [
    "with open(\"chateau.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# remove page numbers\n",
    "txt = re.sub(r\"–\\s*\\d+\\s*–\\n\", \"\", txt)\n",
    "\n",
    "# fix split words\n",
    "txt = re.sub(r\"-\\n\", \"\", txt)\n",
    "\n",
    "# remove line breaks\n",
    "txt = re.sub(r\"\\n\", \" \", txt)\n",
    "\n",
    "# use a single type of -\n",
    "txt = re.sub(r\"–\", \"-\", txt)\n",
    "\n",
    "with open(\"clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(txt)\n",
    "\n",
    "print(len(set(txt.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56297c4d-eaf0-4693-a9dd-d0e051a0c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tokenizable.txt\n",
      "  input_format: \n",
      "  model_prefix: tok\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 512\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: tokenizable.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 3826 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=689484\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=94\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 3826 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=359044\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 29084 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 3826\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 15494\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 15494 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9442 obj=11.2226 num_tokens=33198 num_tokens/piece=3.51599\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8143 obj=8.96044 num_tokens=33365 num_tokens/piece=4.09738\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6104 obj=9.0441 num_tokens=35689 num_tokens/piece=5.84682\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6102 obj=9.00362 num_tokens=35687 num_tokens/piece=5.84841\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4575 obj=9.30053 num_tokens=39497 num_tokens/piece=8.63322\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4575 obj=9.24501 num_tokens=39506 num_tokens/piece=8.63519\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3431 obj=9.55916 num_tokens=43956 num_tokens/piece=12.8114\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3431 obj=9.49818 num_tokens=43959 num_tokens/piece=12.8123\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2573 obj=9.89622 num_tokens=48869 num_tokens/piece=18.993\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2573 obj=9.83264 num_tokens=48867 num_tokens/piece=18.9922\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1929 obj=10.2819 num_tokens=54005 num_tokens/piece=27.9964\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1929 obj=10.2177 num_tokens=54002 num_tokens/piece=27.9948\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1446 obj=10.814 num_tokens=59483 num_tokens/piece=41.1362\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1446 obj=10.738 num_tokens=59485 num_tokens/piece=41.1376\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1084 obj=11.2944 num_tokens=64811 num_tokens/piece=59.7887\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1084 obj=11.2178 num_tokens=64813 num_tokens/piece=59.7906\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=813 obj=11.961 num_tokens=69461 num_tokens/piece=85.4379\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=813 obj=11.875 num_tokens=69461 num_tokens/piece=85.4379\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=609 obj=12.6458 num_tokens=74149 num_tokens/piece=121.755\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=609 obj=12.5362 num_tokens=74174 num_tokens/piece=121.796\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=563 obj=12.8243 num_tokens=75696 num_tokens/piece=134.451\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=563 obj=12.7953 num_tokens=75695 num_tokens/piece=134.449\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tok.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tok.vocab\n"
     ]
    }
   ],
   "source": [
    "with open(\"clean.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tmp = f.read()\n",
    "\n",
    "tmp = re.sub(r\"\\s-\\s\", \"\\n- \", txt)\n",
    "tmp = re.sub(r\"(?<!K)\\.\\s\", \".\\n\", txt)\n",
    "\n",
    "with open(\"tokenizable.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(tmp)\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input = \"tokenizable.txt\",\n",
    "    model_prefix = \"tok\",\n",
    "    vocab_size = 512,\n",
    "    model_type = \"unigram\",\n",
    "    character_coverage = 1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d72a6e9-507e-4731-b512-8be3f0fa2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=\"tok.model\")\n",
    "tokens = sp.encode(txt, out_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "030fdf04-9c33-4f4e-b1ea-b1cc37157e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = len(sp)\n",
    "        self.attention_window = 256\n",
    "        self.nheads = 8\n",
    "        self.d_model = self.nheads * 16\n",
    "        self.mlp_size = 4 * self.d_model\n",
    "        self.n_attention_layers = 5\n",
    "        self.dropout = 0.10\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, self.attention_window, self.d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.nheads,\n",
    "            dim_feedforward=self.mlp_size,\n",
    "            activation=\"gelu\",\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=self.n_attention_layers,\n",
    "            norm=nn.LayerNorm(self.d_model),\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(self.d_model, self.vocab_size, bias=False)\n",
    "        self.out.weight = self.embed.weight\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) + self.pos[:, :x.size(1)]\n",
    "        mask = self._causal_mask(x.size(1), x.device)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        return self.out(x)\n",
    "\n",
    "    def _causal_mask(self, size, device):\n",
    "        mask = torch.triu(torch.ones(size, size, device=device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d1b0259-c85b-4dcf-b3e8-4c004adf7339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embed): Embedding(512, 128)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-4): 5 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=128, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "temperature = 0.8\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c406ce6-f7b9-4187-bfc3-49ff249eb273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SAMPLE] Il était tard lorsque K. arriva. Une neige épaisse couvrait le village. La colline était cachée par la brume et par la nuit, nul rayon de lumière n’indiquait le grand Château. K. resta longtemps sur le pont de bois qui menait de la grand-route au village, les yeux levés vers ces hauteurs qui semblaient vides. Puis il alla chercher un gîte ; les gens de l’auberge n’étaient pas encore au lit ; on n’avait pas de chambre à louer, mais, surpris et déconcerté par ce client qui venait si tard, l’aubergiste lui proposa de le faire coucher sur une paillasse dans la salle. K. accepta. Il y avait encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore encore \n",
      "\n",
      "\n",
      "[SAMPLE] là quelques paysans attablés autour de leurs chopes, mais, ne voulant parler à personne, il alla chercher lui-même la paillasse au grenier et se coucha près du poêle. Il faisait chaud, les paysans se taisaient, il les regarda encore un peu entre ses paupières fatiguées puis s’endormit. Mais il ne tarda pas à être réveillé ; l’aubergiste se tenait debout à son chevet en compagnie d’un jeune homme à tête d’acteur qui avait des yeux minces, de gros sourcils, et des habits de citadin. Les paysans étaient toujours là, quelques-uns avaient fait tourner leurs chaises pour mieux voir. Le jeune homme s’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’ \n",
      "\n",
      "\n",
      "[SAMPLE] excusa très poliment d’avoir réveillé K. et se présenta comme le fils du portier du Château, puis déclara : « Ce village appartient au Château ; y habiter ou y passer la nuit c’est en quelque sorte habiter ou passer la nuit au Château. Personne n’en a le droit sans la permission du comte. Cette permission vous ne l’avez pas ou du moins vous ne l’avez pas montrée. » K. s’étant à moitié redressé passa la main dans ses cheveux pour se recoiffer, leva les yeux vers les deux hommes et dit : - Dans quel village me suis-je égaré ? Y a-t-il donc ici un Château ? - Mais oui, dit le jeune homme lentement, et quelquesssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(3):\n",
    "    start = i * model.attention_window\n",
    "    end = start + model.attention_window\n",
    "    subset = tokens[start:end]\n",
    "    x = torch.tensor([subset], dtype=torch.long)\n",
    "    for _ in range(200):\n",
    "        x = torch.tensor([subset[-model.attention_window:]], dtype=torch.long, device=device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "        probs = torch.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "        subset.append(next_id)\n",
    "    print(\"\\n[SAMPLE]\", sp.decode(subset), \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad7102-cc91-43c5-9bf9-85aac92ba3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | loss=14.21438\n"
     ]
    }
   ],
   "source": [
    "seq_in, seq_out = model.attention_window, model.attention_window // 2\n",
    "stride = seq_out\n",
    "\n",
    "inputs, targets = [], []\n",
    "for i in range(0, len(tokens) - seq_in - seq_out, stride):\n",
    "    chunk = tokens[i : i + seq_in + seq_out]\n",
    "    inputs.append(chunk[:seq_in])\n",
    "    targets.append(chunk[seq_in:])\n",
    "\n",
    "X = torch.tensor(inputs, dtype=torch.long)\n",
    "Y = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-4,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for Xb, Yb in loader:\n",
    "        Xb, Yb = Xb.to(device), Yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xb)\n",
    "        loss = criterion(\n",
    "            logits[:, -seq_out:, :].reshape(-1, model.vocab_size),\n",
    "            Yb.reshape(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if(epoch + 1 < 10):\n",
    "        print(f\"Epoch {epoch+1} | loss={total_loss/len(loader):.5f}\")\n",
    "        \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1} | loss={total_loss/len(loader):.5f}\")\n",
    "        model.eval()\n",
    "        subset = tokens[:model.attention_window]\n",
    "        x = torch.tensor([subset], dtype=torch.long)\n",
    "        for _ in range(200):\n",
    "            x = torch.tensor([subset[-model.attention_window:]], dtype=torch.long, device=device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(x)\n",
    "            probs = torch.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "            next_id = torch.multinomial(probs, 1).item()\n",
    "            subset.append(next_id)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(\"\\n[SAMPLE]\", sp.decode(subset), \"\\n\")\n",
    "        torch.save(model.state_dict(), f\"gpt_epoch_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c481397-b441-46b0-a7ee-afa032de66e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT()\n",
    "model.load_state_dict(torch.load(\"gpt_epoch_99.pt\"))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "temperature = 0.9\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226106b8-a258-48e3-aa0e-d7b457cd5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "temperature = 0.9\n",
    "\n",
    "for i in range(3):\n",
    "    start = i * model.attention_window\n",
    "    end = start + model.attention_window\n",
    "    subset = tokens[start:end]\n",
    "    x = torch.tensor([subset], dtype=torch.long)\n",
    "    for _ in range(200):\n",
    "        x = torch.tensor([subset[-model.attention_window:]], dtype=torch.long, device=device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "        probs = torch.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "        subset.append(next_id)\n",
    "    print(\"\\n[SAMPLE]\", sp.decode(subset), \"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

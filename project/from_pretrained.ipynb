{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f549555e-3e8c-4cd3-b3b9-3c9baacae09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70810a-504b-4239-b0d3-2bfc0c74a5da",
   "metadata": {},
   "source": [
    "# Load a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123adf3-afe0-4ae7-9ab8-2df62eb81f72",
   "metadata": {},
   "source": [
    "## Load from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb0d7e-0764-4232-ab69-937e425c3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32\n",
    ").to(device)\n",
    "\n",
    "tokenizer.save_pretrained(\"tinyllama\")\n",
    "model.save_pretrained(\"tinyllama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8dea74-e1bf-4f72-92f9-f5416e8cf361",
   "metadata": {},
   "source": [
    "## Load from local repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b526e6-3b55-452d-9562-5a7934682ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tinyllama\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./tinyllama\", torch_dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874b4e4-10d5-498b-ad04-228fdb0cfb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Il était tard lorsque  K. arriva. Une neige épaisse couvrait le village. La colline était cachée par la brume et par la nuit, nul rayon de lumière n’indiquait le grand Château.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c7fdc-be91-4a8a-8e83-450494e31c79",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72dbf765-5584-4068-989d-d0f007c0ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chateau.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# remove page numbers\n",
    "txt = re.sub(r\"–\\s*\\d+\\s*–\\n\", \"\", txt)\n",
    "\n",
    "# fix split words\n",
    "txt = re.sub(r\"-\\n\", \"\", txt)\n",
    "\n",
    "# remove line breaks\n",
    "txt = re.sub(r\"\\n\", \" \", txt)\n",
    "\n",
    "# use a single type of -\n",
    "txt = re.sub(r\"–\", \"-\", txt)\n",
    "\n",
    "with open(\"clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc07660-5eb8-4885-8c2b-1caf3da07161",
   "metadata": {},
   "source": [
    "## Fine-tune on Le Château by Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ea228-6763-4805-bd3d-b27bd45f9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"clean.txt\", encoding=\"utf-8\").read()\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "seq_len = 128\n",
    "stride = seq_len // 2\n",
    "total_steps = (len(tokens) - seq_len - 1) // stride\n",
    "print(total_steps)\n",
    "\n",
    "log_path = \"tinyllama_kafka.log\"\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as log:\n",
    "    log.write(\"step,loss,lr\\n\")\n",
    "\n",
    "inputs, targets = [], []\n",
    "for i in range(0, len(tokens) - seq_len - 1, stride):\n",
    "    chunk = tokens[i : i + seq_len + 1]\n",
    "    inputs.append(chunk[:-1])\n",
    "    targets.append(chunk[1:])\n",
    "\n",
    "X = torch.tensor(inputs, dtype=torch.long)\n",
    "Y = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "loader = DataLoader(TensorDataset(X, Y), batch_size=2, shuffle=True)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "warmup_steps = int(total_steps * 0.05)\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[\n",
    "        LinearLR(optimizer, start_factor=0.1, total_iters=warmup_steps),\n",
    "        CosineAnnealingLR(optimizer, T_max=total_steps - warmup_steps, eta_min=1e-7),\n",
    "    ],\n",
    "    milestones=[warmup_steps]\n",
    ")\n",
    "scaler = GradScaler()\n",
    "model.train()\n",
    "\n",
    "total_loss, step = 0, 0\n",
    "\n",
    "for Xb, Yb in loader:\n",
    "    step += 1\n",
    "    Xb, Yb = Xb.to(device), Yb.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with autocast():\n",
    "        outputs = model(Xb, labels=Yb)\n",
    "        loss = outputs.loss\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    scheduler.step()\n",
    "    total_loss += loss.item()\n",
    "    avg_loss = total_loss / step\n",
    "    lr_now = scheduler.get_last_lr()[0]\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"step {step} | loss {avg_loss:.10f} | lr {lr_now:.2e}\\n\")\n",
    "\n",
    "torch.save(model.state_dict(), f\"tinyllama_kafka.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56919f9-d40c-4f68-887f-df2f66348cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tinyllama\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./tinyllama\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "state_dict = torch.load(f\"tinyllama_kafka.pt\", map_location=device)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6072ee-2923-4019-ab1a-a1ab25352c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Il était tard lorsque  K. arriva. Une neige épaisse couvrait le village. La colline était cachée par la brume et par la nuit, nul rayon de lumière n’indiquait le grand Château.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

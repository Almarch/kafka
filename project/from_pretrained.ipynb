{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f549555e-3e8c-4cd3-b3b9-3c9baacae09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "import re\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70810a-504b-4239-b0d3-2bfc0c74a5da",
   "metadata": {},
   "source": [
    "# Load a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123adf3-afe0-4ae7-9ab8-2df62eb81f72",
   "metadata": {},
   "source": [
    "## Load from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb0d7e-0764-4232-ab69-937e425c3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\"tinyllama\")\n",
    "model.save_pretrained(\"tinyllama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8dea74-e1bf-4f72-92f9-f5416e8cf361",
   "metadata": {},
   "source": [
    "## Load from local repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b526e6-3b55-452d-9562-5a7934682ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./tinyllama\"\n",
    ")\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./tinyllama\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b874b4e4-10d5-498b-ad04-228fdb0cfb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il était tard lorsque  K. arriva. Une neige épaisse couvrait le village. La colline était cachée par la brume et par la nuit, nul rayon de lumière n’indiquait le grand Château.\n",
      "\n",
      "Et, lorsque l’inspecteur arriva, tout était bien. Les portes et les fenêtres de la maison étaient fermées. Les volets et les fenêtres étaient fermés. Le chauffage et la lumière étaient désactivés. Les pattes de loup, avec leurs gants d’arme, étalées sur la table, ne faisaient aucune lumière. L’inspecteur K. s’assit à la table et regarda le visage du chien, avec une expression de désespoir.\n",
      "\n",
      "L’ins\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Il était tard lorsque  K. arriva. Une neige épaisse couvrait le village. La colline était cachée par la brume et par la nuit, nul rayon de lumière n’indiquait le grand Château.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "base.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = base.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c7fdc-be91-4a8a-8e83-450494e31c79",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dbf765-5584-4068-989d-d0f007c0ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chateau.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# remove page numbers\n",
    "txt = re.sub(r\"–\\s*\\d+\\s*–\\n\", \"\", txt)\n",
    "\n",
    "# fix split words\n",
    "txt = re.sub(r\"-\\n\", \"\", txt)\n",
    "\n",
    "# remove line breaks\n",
    "txt = re.sub(r\"\\n\", \" \", txt)\n",
    "\n",
    "# use a single type of -\n",
    "txt = re.sub(r\"–\", \"-\", txt)\n",
    "\n",
    "with open(\"clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6af6191-b8fb-45db-b289-4adeebfe6536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (210396 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "txt = open(\"clean.txt\", encoding=\"utf-8\").read()\n",
    "tokens = tokenizer.encode(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc07660-5eb8-4885-8c2b-1caf3da07161",
   "metadata": {},
   "source": [
    "## Fine-tune on Le Château by Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f4dcb-cad4-49a7-9d4c-2b4104127903",
   "metadata": {},
   "source": [
    "### Prepare LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2ea228-6763-4805-bd3d-b27bd45f9f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf34a3-b307-4d46-aaa5-81aa021b6cb9",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94e1d526-b24e-4d79-bc66-a59f66965801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1643\n"
     ]
    }
   ],
   "source": [
    "seq_len = 128\n",
    "stride = seq_len // 2\n",
    "\n",
    "inputs, targets = [], []\n",
    "for i in range(0, len(tokens) - seq_len - 1, stride):\n",
    "    chunk = tokens[i:i + seq_len + 1]\n",
    "    inputs.append(chunk[:-1])\n",
    "    targets.append(chunk[1:])\n",
    "\n",
    "X = torch.tensor(inputs, dtype=torch.long)\n",
    "Y = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# split : premier batch pour validation\n",
    "val_X, val_Y = X[:1], Y[:1]\n",
    "train_X, train_Y = X[1:], Y[1:]\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_X, train_Y), batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_X, val_Y), batch_size=1, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "total_steps = len(train_loader)\n",
    "print(total_steps)\n",
    "warmup_steps = int(total_steps * 0.05)\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[\n",
    "        LinearLR(optimizer, start_factor=0.1, total_iters=warmup_steps),\n",
    "        CosineAnnealingLR(optimizer, T_max=total_steps - warmup_steps, eta_min=1e-6),\n",
    "    ],\n",
    "    milestones=[warmup_steps],\n",
    ")\n",
    "\n",
    "log_path = \"tinyllama_kafka.log\"\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as log:\n",
    "    log.write(\"step,loss,lr\\n\")\n",
    "\n",
    "model.train()\n",
    "total_loss = 0\n",
    "\n",
    "for step, (Xb, Yb) in enumerate(train_loader, start=1): # keep the first one for validation\n",
    "    Xb, Yb = Xb.to(device), Yb.to(device)\n",
    "    outputs = model(Xb, labels=Yb)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    avg_loss = total_loss / step\n",
    "    lr_now = scheduler.get_last_lr()[0]\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        with open(log_path, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"{step},{avg_loss:.6f},{lr_now:.2e}\\n\")\n",
    "            \n",
    "model.save_pretrained(\"tinyllama_kafka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987d7d8-b436-47a6-86ca-97f8c23b30e1",
   "metadata": {},
   "source": [
    "## convert to weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d9b5d-5b04-482b-b152-5d3aa8816741",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tinyllama\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base,\n",
    "    \"tinyllama_kafka\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "torch.save(model.state_dict(), \"tinyllama_kafka.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f981251-d358-46b6-b1e7-23b5502f8de0",
   "metadata": {},
   "source": [
    "## load the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56919f9-d40c-4f68-887f-df2f66348cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./tinyllama\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./tinyllama\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"tinyllama_kafka.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f52b38cb-e928-43b5-acc3-7b32621fb78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tinyllama_kafka\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6072ee-2923-4019-ab1a-a1ab25352c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il était tard lorsque  K. arriva. Une neige épaisse couvrait le village. La colline était cachée par la brume et par la nuit, nul rayon de lumière n’indiquait le grand Château. ilait le en len, il avait parment lai une, seil des et et les’aires, avaité quil lait’’’ le. ler’’ -’- Il’é à. - Il’, K -’’ K -’ - K - -, le - de la, - il sait que. K nea jamaiséc le,’ l-aé,ant leait’ le - son’,’’ -’’ -’ ililait’ à’ qui’ il était. le’, s’a l -, - ne a été n à ne pas - pas il - aé par’ en de. le,’ Ka à - l,. ce qui’’’ de. qu le avait été le- - ?’ -, l’ -’. il neait - qui le a pasé et en’é pour -.,, K,’’ - qui a été’., - K,’ - lait -’ -, qui -’,’ -. ce’ - je lui, -’. le ne’, -i dit -.’ -’ -, K, - vous dit - -, -’ -. - vous nez pas’, vousous - lût, lil’. vous,- vous -, - - n - - dezendre le - ; nousions à, je suis moi,’,,’ K,.,. lui la’. ne nest pas - moi’.’ K -, - le, sa - son - en,. - -’ -,’ -’ - -, K,’ -,’ --’ -, le - de’.’ - le -’ -’ -,’ K -, - je mais’, neais -’ de’ - - je -’ - -. je vois’, moiai’, jeais,’’ - qui - - moi,’ -’ - -’ -,, la, -, -,- c jeais voir queis, - meise - - -- - -’ -,’ - l est de -,.’ - - -, -, -, -, -, je ne’ moiis même c,, -, -’ - - -.’ - -,,’ - l aéiée -. -’ - - -,, - -, -, -, -’ -. -’ -,’ - - -’ -’ - l a’ié, -, -, il nestici,’ - le l - - -, s,’’ -,, -., - l’ ne esti n l un tout -,, mais, il n - pas,. est-,’ neest,’’-, -,’,,, -, jeis la’. il’, -’, -, -,, l - je,,,- - le, - quiilait l, -.’’, -, -, je’,’ - -’,’ - - - -,’ -’, - -, -, -, -, -, -, -, -,’ -, -, -, -, -, -, -, - -, -, -, -,, -, -,, -, - -,, -,, -,, -,, - - -, -, - -,, -, - - - -, -, - -, - -, - -,, - -,, - -, -, -, -, -, -,,, -,,,, -, - -, -, - -, -, -, -,, -,,,, -, - -, - -, - -, -,,,,,, -, -, - - - - - - - - - - - - - -, -, - -,,, - - -, - - -, - - -, - - -,, - - -, - - -, - - -,, - - -,,,, - - -,,, -, - - - - - -, - - -, - - -, - - -, - - -, - - -, -, - -,, - - -, - - -,, - -,, - - -,,,, -, -,, -, -, -, -,, -, -,, -, -,,, -, -,, -, -,, -, - - - - - -,, - - - -, -,, -, - -\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Il était tard lorsque  K. arriva. Une neige épaisse couvrait le village. La colline était cachée par la brume et par la nuit, nul rayon de lumière n’indiquait le grand Château.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1000,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a491c-fb3a-4028-8acc-f2decc1c772d",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979aabc0-5717-4e4f-b6ef-fde940ff6f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.6627% of weights have been fine-tuned\n"
     ]
    }
   ],
   "source": [
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./tinyllama\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "total_elems, diff_elems = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (name, p1), (_, p2) in zip(base.state_dict().items(), model.state_dict().items()):\n",
    "        a = p1.detach().cpu().float().numpy()\n",
    "        b = p2.detach().cpu().float().numpy()\n",
    "        total_elems += a.size\n",
    "        diff_elems += np.sum(np.abs(a - b) > 1e-5)\n",
    "\n",
    "ratio = 100 * diff_elems / total_elems\n",
    "print(f\"{ratio:.4f}% of weights have been fine-tuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

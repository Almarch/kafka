{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f549555e-3e8c-4cd3-b3b9-3c9baacae09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70810a-504b-4239-b0d3-2bfc0c74a5da",
   "metadata": {},
   "source": [
    "# Load a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123adf3-afe0-4ae7-9ab8-2df62eb81f72",
   "metadata": {},
   "source": [
    "## Load from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20bb0d7e-0764-4232-ab69-937e425c3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\"tinyllama_f16\")\n",
    "model.save_pretrained(\"tinyllama_f16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8dea74-e1bf-4f72-92f9-f5416e8cf361",
   "metadata": {},
   "source": [
    "## Load from local repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3b526e6-3b55-452d-9562-5a7934682ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./tinyllama_f16\"\n",
    ")\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./tinyllama_f16\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b874b4e4-10d5-498b-ad04-228fdb0cfb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il était tard lorsque  K. arriva. Une neige épaisse couvrait le village. La colline était cachée par la brume et par la nuit, nul rayon de lumière n’indiquait le grand Château. Il y avait quelques maisons de pierre qui semblaient proches du château, mais elles étaient loin d’être en contact direct avec la structure principale.\n",
      "\n",
      "Ces endroits étaient un peu étranges. La nuit était profonde et la neige les isolait de la vie quotidienne, mais ils n’avaient pas l’impression de vivre de leur propre chair.\n",
      "\n",
      "Le père, K., savait que la neige était un excellent matériau pour les tapis. Il avait besoin de tapis de neige pour le restaurant qui faisait partie du complex\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Il était tard lorsque  K. arriva. Une neige épaisse couvrait le village. La colline était cachée par la brume et par la nuit, nul rayon de lumière n’indiquait le grand Château.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "base.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = base.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c7fdc-be91-4a8a-8e83-450494e31c79",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72dbf765-5584-4068-989d-d0f007c0ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chateau.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# remove page numbers\n",
    "txt = re.sub(r\"–\\s*\\d+\\s*–\\n\", \"\", txt)\n",
    "\n",
    "# fix split words\n",
    "txt = re.sub(r\"-\\n\", \"\", txt)\n",
    "\n",
    "# remove line breaks\n",
    "txt = re.sub(r'\\n(?!–)', ' ', txt)\n",
    "\n",
    "with open(\"clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

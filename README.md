# Kafka-trained LLM

<img alt="Llama" src="https://github.com/user-attachments/assets/91f06e0b-7c79-4de9-9386-8dab581f8289" width="400px" align="right"/>

The goal of this project is to fine-tune a LLM on French litterature, and especially to imprint it with a single book which is the French translation of Kafka's novel: the Castle. The author never ended this book, so the idea is to prolongate it with new chapters.

It is designed for a Nvidia GPU with at least 12 Go VRAM. The [Nvidia container toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) is needed. A minimal 32Go RAM is safe for the data preparation step.

It takes as input:

- a free-to-operate base model: [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama_v1.1), a 1.1B parameters with a 2048 tokens context window.
- a French copyleft translation of The Castle by Kafka, available [here](https://ekladata.com/QAPtMO27HuI4V0hLEhOUd3sv0Nw/Kafka-Le-Chateau.pdf).
- a French literature dataset: [Gallica](https://huggingface.co/datasets/PleIAs/French-PD-Books) for French language enrichment and narrative consistency.

## ğŸ³ Launch

Build & launch the docker container:

```sh
docker build -t kafka .
docker run -d \
  --name kafka \
  -v ./project:/project \
  -p 8000:8000 \
  --gpus all \
  kafka
```

The project is supported by a notebook, available at [port 8000](http://localhost:8000) once the container runs. This notebook may be used to monitor the training and test the models.

## ğŸ§ª Experiment 1

### Step 1 - Resource collection

This step collects the HuggingFace resources: the model, and the training dataset.

- `docker exec -it kafka python 0_data_collection.py`

A text sample generated from TinyLlama is provided. The initial prompt will always be the same: `K. ouvrit la porte.` (*K. opened the door.*), and the model used is always q8-quantized. 500 tokens are generated with a temperature of `0.8`, a repetition penalty of `1.1` and top-p of `0.9`. An English translation has been provided using ChatGPT, asking for close translation including errors and inconsistencies.

<details><summary>ğŸ‡«ğŸ‡·</summary>

```txt
â€” Je ne le crois pas, Maman.

â€” Tu veux qu'il te donne un coup d'Å“il ?

â€” Oh, non, pas de tout cela ! Je viens de voir une femme qui savait bien ce qu'elle avait Ã  dire...

Il ne lÃ¢cha pas la main.

â€” Si tu avais vu des femmes, tu aurais vu des hommes aussi, mon fils !

â€” Quoi ?

â€” Si tu as vu des femmes, tu es vraiment de mauvaise humeur ; si tu as vu des hommes, tu es bien fort de meme, monsieur ! Tu ne crois pas que tu puises les femmes dans tes rÃªves ?

Le pÃ¨re se tourna vers lui :

â€” MaÃ®tre, tu es vraiment un homme. C'est ce que je disais au premier jour de ta vie. Tu n'as pas pris garde Ã  moi. Mais toi, tu n'aimes pas la fille. Elle vous fait peur, tu l'aurais prÃ©fÃ©rÃ©e au docteur Culpepper, lequel vous connaissait depuis longtemps et dont il fallait se tuer pour s'en rendre compte.

Mademoiselle Chatillon reÃ§ut une telle rÃ©ponse en sortant, elle se mit Ã  la promener avec son petit cheval blanc, elle acheva de se rappeler cette scÃ¨ne de ses rÃªveries : c'Ã©tait une sorte de contrainte, le docteur Culpepper avait dit qu'elle devrait Ãªtre folle.

AprÃ¨s cette nuit de noces, Chatillon Ã©tait de retour chez sa mÃ¨re, puis il partit Ã  Paris. Il eut pour compagnons de voyage M. et Mademoiselle Houdon et Monsieur de la Tour du Pin. Il y avait Ã©galement un jeune officier, le marÃ©chal de Campard. Au bout de quelques semaines, le comte de Montalivet fit venir son fils pour le conduire jusqu'Ã  Paris.

Chatillon fut accueilli par une foule entiÃ¨re qui allait enfin admettre sa beautÃ© et sa bontÃ©, mais aussi la grande cruautÃ© de son goÃ»t envers les femmes, car
```
</details>

<details><summary>ğŸ‡¬ğŸ‡§</summary>
  
```txt
â€” I don't believe it, Mom.

â€” You want him to give you a glance?

â€” Oh, no, nothing of that! I just saw a woman who knew well what she had to say...

He didnâ€™t let go of the hand.

â€” If you had seen women, you would have seen men too, my son!

â€” What?

â€” If youâ€™ve seen women, youâ€™re really in a bad mood; if youâ€™ve seen men, youâ€™re very strong the same, sir! Donâ€™t you think you can draw women from your dreams?

The father turned toward him:

â€” Master, you really are a man. Thatâ€™s what I was saying on the first day of your life. You didnâ€™t take heed of me. But you, you donâ€™t love the girl. She frightens you, you would have preferred her to Doctor Culpepper, who had known you for a long time and for whom one had to kill oneself to realize it.

Miss Chatillon received such an answer on going out, she began to walk it around with her little white horse, she finished remembering this scene from her reveries: it was a kind of constraint, Doctor Culpepper had said she would have to be crazy.

After this wedding night, Chatillon was back at his motherâ€™s, then he left for Paris. His travel companions were Mr. and Miss Houdon and Monsieur de la Tour du Pin. There was also a young officer, Marshal Campard. After a few weeks, the Count of Montalivet sent for his son to lead him to Paris.

Chatillon was welcomed by an entire crowd that would at last admit his beauty and his goodness, but also the great cruelty of his taste toward women, for
```
</details>

### Step 2 - French literature aculturation

The goal of this step is to reorient the base model towards a generator of French literature. It consists in a full-weight training over 1M samples of 512 tokens from the Gallica collection. The model should forget its chatbot abilities, its multilinguism and its coding knowledge; to learn about *boudoir intrigues* and French classical literature content and form.

- `docker exec -it kafka python 1a_prepare_gallica_1M_512t.py`
- `docker exec -it kafka python 2a_train_tinyllama_fullweight_gallica_512t.py`

<div align="center">
<img width="500" alt="plot_train_fullweight" src="https://github.com/user-attachments/assets/3fa3ba32-f0e2-47e2-ad92-eb662e9128dd" />
</div>

Generation sample at this step (same parameterization than step 1):

<details> <summary>ğŸ‡«ğŸ‡·</summary>
  
```txt
â€” 5 â€” 
Qu'est-ce que cela? demanda l'homme d'Elsen. â€” Un homme. â€” Et quel est cet homme? â€” Un des plus riches du royaume, un noble. â€” Vous allez le voir ? â€” Oui, mais il n'y a pas longtemps qu'il est parti... Nous sommes au fond de la forÃªt. 
Oui, dit l'un d'eux, il y avait cinq jours que nous avions passÃ© en courant sur cette route. 
â€” Votre maÃ®tre est un brave et un honnÃªte homme. â€” Vous savez ce qui vous sera arrivÃ©. â€” Ce que j'aime Ã  appeler une vertu, c'est de ne jamais se faire tromper. â€” Mais voici une fois qui m'est impossible. â€” Et qui donc? â€” Un noble de la maison d'Erthor. 
Cette question Ã©tait Ã©nigmatique; il n'Ã©tait point de la sagesse de M. 

â€” 6 â€” Lalande pour lui rÃ©pondre : Â« Un noble de la maison d'Erthor. Â» Cela est bien Ã©trange, car M. Lalande ne connaissait pas les gens de son pays, il Ã©tait nÃ© dans une province, on ne l'avait pas vu, ni avec lequel on ne s'Ã©tait pas fait connaÃ®tre, et sa maison avait Ã©tÃ© dÃ©truite par les Anglais, par les Suisses, par ceux qui Ã©taient venus aprÃ¨s eux ; enfin, son pÃ¨re n'avait jamais vÃ©cu et son nom, lui aussi, n'Ã©tait connu que par ses parents ; il Ã©tait nÃ© en un village de Bourgogne, et comme il Ã©tait tout en deuil, il l'avait donnÃ© Ã  son pÃ¨re, son Ã©pouse Ã©tait morte dans un grand accident, ils avaient eu quatre enfants, deux fils et deux filles, et il Ã©tait tombÃ© en laissant un seul enfant, une fille qui a trouvÃ© une place chez un noble de la maison d'Erthor. â€” 

â€” 7 â€” Qui peut Ãªtre celui de ces trois hommes? 
â€” Eh bien ! je le crois, sans doute, parce qu'une autre personne que celle qui a fait cette demande
```
</details>

<details><summary>ğŸ‡¬ğŸ‡§</summary>
  
```txt
â€” 5 â€”
â€œWhat is that?â€ asked the man from Elsen.
â€” â€œA man.â€
â€” â€œAnd what sort of man is that?â€
â€” â€œOne of the richest in the kingdom, a noble.â€
â€” â€œYouâ€™re going to see him?â€
â€” â€œYes, but he hasnâ€™t been gone longâ€¦ We are at the bottom of the forest.â€
â€œYes,â€ said one of them, â€œit had been five days that we had run along this road.â€
â€” â€œYour master is a brave and an honest man.â€
â€” â€œYou know what will have happened to you.â€
â€” â€œWhat I like to call a virtue is never letting oneself be deceived.â€
â€” â€œBut here is one time that is impossible for me.â€
â€” â€œAnd who then?â€
â€” â€œA noble of the house of Erthor.â€

This question was enigmatic; it was not in Mr.

â€” 6 â€” Lalandeâ€™s wisdom to answer him: â€œA noble of the house of Erthor.â€ That is very strange, for Mr. Lalande did not know the people of his own country, he had been born in a province, he had not been seen, nor had he made himself known to anyone, and his house had been destroyed by the English, by the Swiss, by those who had come after them; finally, his father had never lived and his name, too, was known only by his relatives; he had been born in a village of Burgundy, and as he was all in mourning, he had given it to his father, his wife had died in a great accident, they had had four children, two sons and two daughters, and he had fallen leaving only one child, a daughter who found a place with a noble of the house of Erthor. â€”

â€” 7 â€” â€œWho can that be, among these three men?â€
â€” â€œWell! I believe it, no doubt, because another person than the one who made this requestâ€¦â€
```
</details>

### Step 3 - Strengthen the narrative arc

This step aims at teaching the model long (2048 tokens) and consistent narrative arcs, which is essential for a literature project. However, because the VRAM need increases quadratically with the context window, a QLoRA approach is undertaken from this step (and for the next one). LoRA adapters are trained over 100M samples of 2048, still from the Gallica collection.

- `docker exec -it kafka python 1b_prepare_gallica_100K_2048t.py`
- `docker exec -it kafka python 2b_train_gallica_2048t_QLoRA.py`

<div align="center">
<img width="500" alt="plot_train_qlora" src="https://github.com/user-attachments/assets/22440696-fd6e-494e-a348-ce4deefda5c6" />
</div>

Generation sample at this step (same parameterization than step 1):

<details> <summary>ğŸ‡«ğŸ‡·</summary>
  
```txt
BARONIS.
A la bonne heure, mon ami ! 
L'entendÃ®mes Ã  demi-voix; puis je me levai pour l'accompagner Ã  sa chambre. 
MADAME HERMINE. 
Oh! si vous vouliez bien me suivre ; j'aurais un peu plus de confiance en votre amitiÃ©. 
(BARONIS entra dans la chambre de la comtesse.) 
BARONIS. 
Oui, monsieur, et j'attends un moment avec vous ? 

MOIRELLA. 
Pour vous raconter le rÃ©cit de ces aventures qui m'ont Ã©tÃ© si vifement intÃ©ressantes dans son cours, mon cher monsieur. 
BARONIS. 
Je ne suis pas encore parvenu Ã  les lire. 
MOIRELLA. 
Et il est assez difficile d'Ãªtre rÃ©ellement Ã©mu en voyant ce qu'on fait au public, sans avoir l'habitude du thÃ©Ã¢tre et de l'art dramatique. 
BARONIS. 
Il y a quelque temps que j'ai eu le plaisir de voir cette tragÃ©die jouÃ©e devant mes camarades du corps militaire de Paris : c'est une production remarquable de M. ThÃ©odore de Banville, qui n'a rien d'Ã©tonnant au premier abord, car elle a tous les ressemblances avec la piÃ¨ce que nous venons d'Ã©couter. Il se trouve en effet que ce sont des personnages de la vie moderne, des figures des classes supÃ©rieures, des scÃ¨nes du grand monde, d'une langue simple et concise, de l'art de s'exprimer comme l'on ne peut le faire dans d'autres circonstances, avec plus de finesse et de rÃ©alisme. 
MOIRELLA. 
Vous avez bien raison, monsieur; mais vous Ãªtes trop fort pour Ãªtre obligÃ© de traiter les choses ainsi. 
BARONIS. 
Si je ne tins pas les yeux sur le spectacle, et que ce soit le jeu, je le croirais; et je me dÃ©fie de vous appr
```
</details>

<details><summary>ğŸ‡¬ğŸ‡§</summary>
  
```txt
BARONIS.
At last, my friend!
We heard him in a half-voice; then I got up to accompany him to his room.

MADAME HERMINE.
Oh! if you would be so kind as to follow me; I would have a bit more confidence in your friendship.

(BARONIS entered the countessâ€™s room.)

BARONIS.
Yes, sir, and am I waiting a moment with you?

MOIRELLA.
To tell you the story of those adventures that were so vividly interesting to me as they happened, my dear sir.

BARONIS.
I have not yet managed to read them.

MOIRELLA.
And it is rather difficult to be truly moved when seeing what is shown to the public, without being used to the theatre and the dramatic art.

BARONIS.
Some time ago I had the pleasure of seeing that tragedy played before my comrades of the military corps of Paris: it is a remarkable work by Mr. ThÃ©odore de Banville, which is not surprising at first sight, for it has all the resemblances with the play we have just listened to. Indeed, it turns out they are characters of modern life, figures of the upper classes, scenes of high society, with a simple and concise language, with the art of expressing oneself as one cannot do in other circumstances, with more subtlety and realism.

MOIRELLA.
You are quite right, sir; but you are too strong to be forced to handle things in that way.

BARONIS.
If I did not keep my eyes on the show, and if it were just the acting, I would believe it; and I mistrust telling you
```
</details>

### Step 4 - Stylistic imprinting on Kafka

Finally, the French litterature model and more specifically its previously pre-trained LoRA are fine-tuned on the target book: the French translation of The Castle by Kafka. This step takes as input 2048 token long sequences of the book, with a stride of 512, yielding 4 shuffled "pseudo-epochs" (each token of the book is seen 4 times).

- `docker exec -it kafka python 1c_prepare_kafka_2048t.py`
- `docker exec -it kafka python 2c_train_kakfa_QLoRA.py`

<div align="center">
<img width="500" alt="plot_train_kafka" src="https://github.com/user-attachments/assets/4338ce8e-8a25-4e49-9a3e-ab97f205c07e" />
</div>

Generation sample at this step (same parameterization than step 1):

<details><summary>ğŸ‡«ğŸ‡·</summary>

```txt
25 
C'Ã©tait la maison de mon oncle, qui Ã©tait mort il y a une annÃ©e environ. Je me trouvais le premier auprÃ¨s de lui dans sa chambre, quand j'aperÃ§us quelqu'un que je ne m'attendais pas, et qui l'attira par la main. Il s'approcha d'eux tous deux et ils mirent les bras autour des corps. Le lendemain matin, je vis mon oncle, comme j'en avais vu autrefois; mais c'Ã©taient ses jambes qui avaient Ã©tÃ© coupÃ©es en deux, et ses cheveux noirs sur son front, ainsi qu'il s'appelle, avec ces cheveux blancs qui tombent aux pieds, un peu grisÃ©s. Ils Ã©taient tous deux Ã  cheval, sur un mouton blanc, et se regardaient attentivement. Un ruisseau coulait devant nous; tout Ã©tait rouge, et un grand nuage de poussiÃ¨re descendait du ciel. Cela n'allait pas assez loin pour qu'il eÃ»t fallu une bouteille de vin pour relever leur tempÃ©rament. 

â€” 26 â€” 
Les derniers jours de dÃ©cembre, les vieilles gens sont dites fort gaielles; car il est plus doux de vivre avec le bonheur d'autrui qu'avec celui de soi-mÃªme. Nous avions Ã  nous remettre ce jour-lÃ , et nous nous Ã©tions rÃ©fugiÃ©s chez un de mes amis, que nous appelions de nos propres yeux monsieur le curÃ©. Ce fut le dernier soir de ma prÃ©sence ici, et je suis venu Ã  Paris pour faire mon voyage. Quelque temps aprÃ¨s, mon oncle et notre ami Ã©tant revenus Ã  leur poste, on fit quelques choses en parlant ensemble. AprÃ¨s avoir pris quelque chose de boisson, ils allÃ¨rent Ã  l'Ã©glise. A peine avait-ils quittÃ© cette ville et ces terres, qu'ils eurent la fiÃ¨vre. J'avais eu le malheur de voir mon oncle malade, mais j'Ã©tais si faible, qu'au moment oÃ¹ il Ã©tait trÃ¨s malade, je croyais
```
</details>

<details><summary>ğŸ‡¬ğŸ‡§</summary>
  
```txt
25
It was my uncleâ€™s house, who had died about a year ago. I was the first beside him in his room, when I noticed someone I was not expecting, and who pulled him by the hand. He came closer to the two of them and they put their arms around the bodies. The next morning, I saw my uncle, as I had seen him before; but it was his legs that had been cut in two, and his black hair on his forehead, as he is called, with those white hairs that fall to the feet, a little greyed. They were both on horseback, on a white sheep, and were looking at each other attentively. A stream was flowing before us; everything was red, and a great cloud of dust was coming down from the sky. It didnâ€™t go far enough for them to need a bottle of wine to lift their spirits.

â€” 26 â€”
In the last days of December, old people are said to be very merry; for it is sweeter to live with someone elseâ€™s happiness than with oneâ€™s own. We had to pull ourselves together that day, and we had taken refuge at one of my friendsâ€™, whom we called with our own eyes Mister CurÃ©. That was the last evening of my presence here, and I came to Paris to make my journey. Some time after, my uncle and our friend having returned to their post, they did some things while talking together. After having taken something to drink, they went to the church. Barely had they left this town and these lands, when they caught the fever. I had had the misfortune of seeing my uncle ill, but I was so weak that at the moment when he was very ill, I thoughtâ€¦
```
</details>

### ğŸ“Š Conclusions

The French literary aculturation (step 1) was very effective. The model completely switched its register, using a highly elevated language register and a specific vocabulary of the French XIXth literature. The number of training samples could have been shortened, as the loss stagnated for more than half the training.

Step 2 and 3 were disappointing. For step 2, either the undertaken QLoRA approach was not effective with this configuration, either the longer attention was not degraded at step 1, in any case no sustantial gain was observed. Step 3 was clearly not daring enough:  loss variations were erratic, and the model failed to generate Castle-related content.

## ğŸ§ª Experiment 2 - Full-weight fine-tuning after aculturation

Given the failure of the QLoRA approach, a full-weight training was directly attempted on the target book. The model from experiment 1 - Step 1, *i.e.* the French literary aculturated model, was taken as the base model for this second experiment.

The training on The Castle was more aggressive: it was split into 512 tokens chunks, shuffled, and used as training material along 10 epochs. The goal of this step is to play with the edge of overfitting, in other words, to deeply imprint the target book without yielding a perfect recitation.

- `docker exec -it kafka python 1d_prepare_kafka_512t.py`
- `docker exec -it kafka python 2d_train_kafka_fullweight_512t.py`

<div align="center">
<img width="500" alt="plot_train_kafka" src="https://github.com/user-attachments/assets/8303429f-cd22-4b83-afc0-087fe3c84990" />
</div>

<details><summary>ğŸ‡«ğŸ‡·</summary>

```txt
â€“ Monsieur lâ€™Arpenteur ? demanda-t-il en se penchant Ã  petits coups dans le lit, sans mÃªme toucher K., mais sans pouvoir sâ€™empÃªcher dâ€™en tirailler les cheveux. Dans quel monde ? Quand je vous vois encore si malade, quand je vous retrouve au milieu de cette nuit froide et vide, quand jâ€™entends entendre mes messieurs, qui sont tous lÃ -bas, discuter de moi comme ils eussent discutÃ© de vous â€“ ils nâ€™ont pas oubliÃ© votre place, mais ils ont fait un gros geste pour ne pas te la lui remettre ! â€“, quand je vous vois ainsi, mÃªme si tu nâ€™Ã©tais plus capable de rien faire, il est donc impossible que tu me rendes compte quâ€™un tel chÃ¢timent ait Ã©tÃ© fait pour toi. Il ne suffit pas de nâ€™Ãªtre pas le premier des gens, il faut aussi dâ€™Ãªtre digne dâ€™y Ãªtre. Je vous ai pris pour un valet de chambre, je nâ€™ai jamais osÃ© croire quâ€™on mâ€™eÃ»t cru capable dâ€™un pareil travail ; câ€™est ce que je disais autrefois mÃªme aux gens qui voulaient mâ€™embaucher, mais maintenant je sais que je vous ai pris pour un valet de chambre, vous avez seulement manquÃ© de qualitÃ©s nÃ©cessaires Ã  cet emploi. Vous nâ€™avez pas su rester longtemps dans une piÃ¨ce oÃ¹ on commenÃ§ait par vous mettre Ã  nu, vous nâ€™aviez pas la seule force de le faire. Câ€™est au fond du moins ma faute, car, quand jâ€™ai appris vos dÃ©fauts, je vous ai mis dans la cour du chÃ¢teau et jâ€™ai fait tout mon possible pour ne pas vous voir. Mais cela ne suffit pas, et il ne peut Ãªtre question que dâ€™une chose plus grande : je suis mÃ©chante avec vous, je vous traite mal, je ne puis me trouver Ã  ta place et nâ€™ai aucun dÃ©sir de devenir celle-lÃ . Tu es donc reparti ?
â€“ Oui, dit K. en regardant devant lui.
â€“ Ã‰trange jeune homme, dit Olga, je lâ€™
```
</details>

<details><summary>ğŸ‡¬ğŸ‡§</summary>

```txt
â€“ â€œLand-Surveyor, sir?â€ he asked, leaning in little dips over the bed, without even touching K., yet unable to stop tugging at his hair. â€œIn what world? When I still see you so sick, when I find you again in the middle of this cold and empty night, when I hear my gentlemen, who are all down there, discussing me as they would have discussed youâ€”they havenâ€™t forgotten your place, but they made a big gesture not to give it back to him!â€”when I see you like this, even if you were no longer capable of doing anything, how is it possible you cannot explain to me that such a punishment was done for you. It isnâ€™t enough not to be the first of people, you must also be worthy of being among them. I took you for a valet, I never dared believe one could have thought me capable of such a piece of work; that is what I used to tell even to people who wanted to hire me, but now I know I took you for a valet, you merely lacked the qualities necessary for that position. You didnâ€™t know how to stay long in a room where they began by stripping you naked, you didnâ€™t have the slightest strength to do it. It is, at bottom, my fault at least, for when I learned your shortcomings, I put you in the castleâ€™s courtyard and did everything in my power not to see you. But that isnâ€™t enough, and there can be talk only of something greater: I am wicked with you, I treat you badly, I cannot put myself in your place and have no desire to become that. So you have gone away again?â€
â€“ â€œYes,â€ said K., staring ahead of him.
â€“ â€œStrange young man,â€ said Olga, â€œIâ€¦â€
```
</details>

### ğŸ“Š Conclusions

This second experiment was clearly more convincing: the characters and vocabulary are recurrently taken from The Castle, and the phrasing is much more Kafkaesque. We stopped at 10 epoch but the loss kept decreasing; there is a gradient of imprinting that may be played with, from zero to pure memorization of the book. 10 epochs likely crosses into the audacious zone, the narrative thread is a bit disjointed and the model sometimes goes off the rails, for instance starting to declaim theatre.
